{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==> WARNING: A newer version of conda exists. <=="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving environment: ...working... done"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  current version: 4.8.3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  latest version: 4.9.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Package Plan ##"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please update conda by running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  environment location: C:\\Users\\Mohit\\anaconda3\\envs\\venv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  added / updated specs:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    $ conda update -n base -c defaults conda"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    - torchtext"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    torchtext-0.7.0            |             py38         2.8 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  torchtext          pytorch/win-64::torchtext-0.7.0-py38\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "torchtext-0.7.0      | 2.8 MB    |            |   0% \n",
      "torchtext-0.7.0      | 2.8 MB    |            |   1% \n",
      "torchtext-0.7.0      | 2.8 MB    | 1          |   2% \n",
      "torchtext-0.7.0      | 2.8 MB    | 3          |   4% \n",
      "torchtext-0.7.0      | 2.8 MB    | 8          |   8% \n",
      "torchtext-0.7.0      | 2.8 MB    | #2         |  13% \n",
      "torchtext-0.7.0      | 2.8 MB    | ##1        |  21% \n",
      "torchtext-0.7.0      | 2.8 MB    | ###9       |  39% \n",
      "torchtext-0.7.0      | 2.8 MB    | ####4      |  44% \n",
      "torchtext-0.7.0      | 2.8 MB    | #######4   |  75% \n",
      "torchtext-0.7.0      | 2.8 MB    | ########5  |  85% \n",
      "torchtext-0.7.0      | 2.8 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "conda install -c pytorch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from onmt.utils.logging import init_logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import onmt.opts as opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from kp_gen_eval import _get_parser\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "import onmt.keyphrase.pke as pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import onmt.keyphrase.kp_inference as kp_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'onmt.keyphrase.kp_inference' from 'D:\\\\OpenNMT-kpg-release\\\\onmt\\\\keyphrase\\\\kp_inference.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(kp_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a text (assume current directory is OpenNMT-kpg/notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\OpenNMT-kpg-release\\test\n",
      "Loaded #(docs)=1\n"
     ]
    }
   ],
   "source": [
    "data_root_path = 'D:\\\\OpenNMT-kpg-release\\\\test'\n",
    "print(os.path.abspath(data_root_path))\n",
    "doc_dicts = []\n",
    "for subdir, dirs, files in os.walk(data_root_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "#             print(filepath)\n",
    "            text = open(filepath, 'r').readlines()\n",
    "            pars = [l for l in text if len(l.split()) > 20]\n",
    "            doc = {'name': file, 'path': filepath, 'text': text, 'paragraph': pars}\n",
    "#             print('#line=%d, #par=%d' % (len(text), len(pars)))\n",
    "            doc_dicts.append(doc)\n",
    "    \n",
    "print('Loaded #(docs)=%d' % (len(doc_dicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "It is the most complex and original idea I have ever seen or heard of. Also, because it delves into the topic of human emotions, but in an artificial way, if I were to be punny and serious all at once. From what I have seen of the show, I believe that we, as human beings, can compare ourselves to the androids, because we can definitely relate to them\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(doc_dicts)-1)\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par_id = random.randint(0, len(doc_dicts[doc_id]['paragraph'])-1)\n",
    "text_to_extract = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(text_to_extract)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Deep Keyphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _get_parser()\n",
    "config_path = '../config/translate/config-rnn-keyphrase.yml'\n",
    "one2one_ckpt_path = '../models/keyphrase/meng17-one2one-kp20k-topmodels/kp20k-meng17-one2one-rnn-BS128-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Covfalse-Contboth-IF1_step_30000.pt'\n",
    "one2seq_ckpt_path = '../models/keyphrase/meng17-one2seq-kp20k-topmodels/kp20k-meng17-verbatim_append-rnn-BS64-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Reusetrue-Covtrue-PEfalse-Contboth-IF1_step_50000.pt'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "setattr(opt, 'models', [one2one_ckpt_path])\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e393ddb978ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m scores, predictions = translator.translate(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0msrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_to_extract\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtgt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msrc_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OpenNMT-kpg-release\\onmt\\translate\\translator.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, src, tgt, src_dir, batch_size, attn_debug, phrase_table, opt)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;31m# modified by @memray to accommodate keyphrase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         data = inputters.str2dataset[self.data_type](\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             readers=([self.src_reader, self.tgt_reader]\n",
      "\u001b[1;32mD:\\OpenNMT-kpg-release\\onmt\\inputters\\keyphrase_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fields, readers, data, dirs, sort_key, filter_pred, tgt_type)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_vocabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mex_dict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_join_dicts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mread_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcan_copy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[0msrc_field\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\OpenNMT-kpg-release\\onmt\\inputters\\keyphrase_dataset.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, sequences, side, _dir)\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0msequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataReaderBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;31m# json_line = json_line.decode(\"utf-8\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m             \u001b[0mjson_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;31m# Note tgt could be a list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "scores, predictions = translator.translate(\n",
    "    src=[text_to_extract],\n",
    "    tgt=None,\n",
    "    src_dir=opt.src_dir,\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")\n",
    "print('Paragraph:\\n\\t'+text_to_extract)\n",
    "print('Top predictions:')\n",
    "keyphrases = [kp.strip() for kp in predictions[0] if (not kp.lower().strip() in stoplist) and (kp != '<unk>' )]\n",
    "for kp_id, kp in enumerate(keyphrases[: min(len(keyphrases), 20)]):\n",
    "    print('\\t%d: %s' % (kp_id+1, kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: anything (0.0000)\n",
      "\t2: anything that (0.0000)\n",
      "\t3: anything that threatens (0.0000)\n",
      "\t4: that (0.0000)\n",
      "\t5: that threatens (0.0000)\n",
      "\t6: threatens (0.0000)\n",
      "\t7: company (0.0000)\n",
      "\t8: ability (0.0000)\n",
      "\t9: ability to (0.0000)\n",
      "\t10: ability to meet (0.0000)\n",
      "\t11: to meet (0.0000)\n",
      "\t12: to meet its (0.0000)\n",
      "\t13: meet (0.0000)\n",
      "\t14: meet its (0.0000)\n",
      "\t15: meet its target (0.0000)\n",
      "\t16: its (0.0000)\n",
      "\t17: its target (0.0000)\n",
      "\t18: its target or (0.0000)\n",
      "\t19: target (0.0000)\n",
      "\t20: target or (0.0000)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'test'\n",
    "dataset_path = '../data/%s/' % dataset_name\n",
    "_ = kp_inference.extract_pke(text_to_extract, method='tfidf' , dataset_path=dataset_path,\n",
    "            df_path=os.path.abspath(dataset_path + '../%s.df.tsv.gz' % dataset_name), top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: called business risk (0.0331)\n",
      "\t2: financial goals (0.0332)\n",
      "\t3: called business (0.0632)\n",
      "\t4: business risk (0.0830)\n",
      "\t5: risk (0.1075)\n",
      "\t6: company (0.1209)\n",
      "\t7: anything (0.1383)\n",
      "\t8: ability (0.1383)\n",
      "\t9: risks may come (0.1415)\n",
      "\t10: business (0.1657)\n",
      "\t11: may (0.1765)\n",
      "\t12: threatens (0.1793)\n",
      "\t13: meet (0.1793)\n",
      "\t14: target (0.1793)\n",
      "\t15: achieve (0.1793)\n",
      "\t16: financial (0.1793)\n",
      "\t17: goals (0.1793)\n",
      "\t18: called (0.1793)\n",
      "\t19: risk management strategy (0.1844)\n",
      "\t20: anything that threatens (0.1851)\n"
     ]
    }
   ],
   "source": [
    "_ = kp_inference.extract_pke(text_to_extract, method='yake', top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n",
      "WARNING:root:Not enough candidates to choose from (10 requested, 6 given)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: risk management (0.1901)\n",
      "\t2: company head (0.1464)\n",
      "\t3: financial goals (0.1464)\n",
      "\t4: risk (0.0950)\n",
      "\t5: sources (0.0732)\n",
      "\t6: company (0.0732)\n"
     ]
    }
   ],
   "source": [
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
