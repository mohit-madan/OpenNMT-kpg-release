# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i-tGDut9EshAU2I8qrumlT03Kf5NfPgp
"""
# Commented out IPython magic to ensure Python compatibility.
# %cd content/gdrive/My Drive/SentenceMatching
# ! ls

from sentence_transformers import SentenceTransformer, util
import torch
import os
from os import listdir
from os.path import isfile, join
import pandas as pd
from tqdm import tqdm
import numpy as np
import math
import random
from torch import nn
import torch.nn.functional as F

random.seed(8)
dr = 'trainSample'
filesList = [dr + "/" + f for f in listdir(dr) if isfile(join(dr, f))]

finalDF = pd.DataFrame()

for file in tqdm(filesList):
    if file.lower().endswith(('.xls', 'xlsx')):
      data_df = pd.read_excel(file, sheet_name=0)
    elif file.lower().endswith('.csv'):
        data_df = pd.read_csv(file)
    else:
        pass
    data_df = data_df.dropna(how='all')
    data_df = data_df.reset_index(drop=True)
    codeCols = [ind for ind, x in enumerate(list(data_df)) if "code" in x.lower()]
    remainCols = [ind for ind, x in enumerate(list(data_df)) if "code" not in x.lower()]
    codesDF = data_df.iloc[:, codeCols]
    remainDF = data_df.iloc[:, remainCols]


    #######################################
    # add separator and replace NaN to empty space
    # convert to lists
    arr = codesDF.add('/').fillna('').values.tolist()
    # list comprehension, replace empty spaces to NaN
    combinedCodes = pd.Series([''.join(x).strip('/') for x in arr]).replace('^$', np.nan, regex=True)
    # replace NaN to None
    combinedCodes = combinedCodes.where(combinedCodes.notnull(), None)
    combinedCodes.name = "keywords"
    ##################################################
    newDF = remainDF.join(combinedCodes)
    newDF['keywords'] = newDF.keywords.apply(lambda x: str(x).split('/'))
    allKeywords = set(newDF['keywords'].apply(pd.Series).stack().reset_index(drop=True).unique())
    newDF['nonKeywords'] = [list(allKeywords - set(newDF['keywords'].iloc[i])) for i in newDF.index]
    finalDF = finalDF.append(newDF, ignore_index=True)

finalDF = finalDF.rename(columns= {'Src': "abstract"}, inplace=False)
# shuffle pd rows randomly
finalDF.sample(frac=1)

finalDF = finalDF.reset_index(drop=True)

totalRows = len(finalDF.index)

train_df = finalDF.iloc[0:math.floor(totalRows*0.7), :]
validate_df = finalDF.iloc[math.floor(totalRows*0.7):math.floor(totalRows*0.8), :]
test_df = finalDF.iloc[math.floor(totalRows*0.8):, :]

train_df

class KeyphraseDataset(torch.utils.data.Dataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, df):
        'Initialization'
        self.responseList = df["abstract"]
        self.correctKeyphraseList = df["keywords"]
        self.wrongKeyphraseList = df["nonKeywords"]

  def __len__(self):
        'Denotes the total number of samples'
        return len(self.responseList)

  def __getitem__(self, index):
        'Generates one sample of data'
        # Select sample

        # Load data and get label
        x1 = self.responseList[index]
        rand = random.random()
        if(rand<=.5):
            x2 = random.choice(self.correctKeyphraseList[index])
            y = 1
        else:
            x2 = random.choice(self.wrongKeyphraseList[index])
            y = 0

        return x1,x2,y

trainDataset = KeyphraseDataset(train_df)
trainLoader = torch.utils.data.DataLoader(trainDataset,batch_size=4, shuffle=True,num_workers=4)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(768*3, 200)
        self.fc2 = nn.Linear(200, 200)
        self.fc3 = nn.Linear(200, 1)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.sigmoid(x)

net = Net()
learning_rate = 0.1
# create a stochastic gradient descent optimizer
optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)
# create a loss function
criterion = nn.BCELoss()

embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

epochs = 2
for epoch in range(epochs):
    for batch_idx, (response, keyword, target) in enumerate(trainLoader):
        response_embedding = embedder.encode(response, convert_to_tensor=True)
        keyword_embedding = embedder.encode(keyword, convert_to_tensor=True)
        diff = torch.abs(response_embedding - keyword_embedding)
        input = torch.cat((diff, response_embedding, keyword_embedding))
        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)
        optimizer.zero_grad()
        net_out = net(input)
        loss = criterion(net_out, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(data), len(train_loader.dataset),
                           100. * batch_idx / len(train_loader), loss.data[0]))

